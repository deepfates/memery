{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Index, query and save embeddings of images by folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rationale\n",
    "\n",
    "**Memery takes a folder of images, and a search query, and returns a list of ranked images.**\n",
    "\n",
    "The images and query are both projected into a high-dimensional semantic space, courtesy of OpenAI's [https://github.com/openai/CLIP](https://openai.com/blog/clip/). These embeddings are indexed and treemapped using the [Annoy](https://github.com/spotify/annoy) library, which provides nearest-neighbor results for the search query. These results are then transmitted to the user interface (currently as a list of file locations).\n",
    "\n",
    "We provide various interfaces for the end user, which all call upon the function `query_flow` and `index_flow` below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modular flow system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memery uses the Neural Search design pattern as described by Han Xiao in e.g. [General Neural Elastic Search and Go Way Beyond](https://hanxiao.io/2019/07/29/Generic-Neural-Elastic-Search-From-bert-as-service-and-Go-Way-Beyond)&c.\n",
    "\n",
    "This is a system designed to be scalable and distributed if necessary. Even for a single-machine scenario, I like the functional style of it: grab data, transform it and pass it downstream, all the way from the folder to the output widget.\n",
    "\n",
    "There are two main types of operater in this pattern: **flows** and **executors**.\n",
    "\n",
    "**Flows** are specific patterns of data manipulation and storage. **Executors** are the operators that transform the data within the flow. \n",
    "\n",
    "There are two core flows to any search system: indexing, and querying. The plan here is to make executors that can be composed into flows and then compose the flows into a UI that supports querying and, to some extent, indexing as well.\n",
    "\n",
    "The core executors for this use case are:\n",
    " - Loader\n",
    " - Crafter\n",
    " - Encoder\n",
    " - Indexer\n",
    " - Ranker\n",
    " - Gateway\n",
    " \n",
    "\n",
    "**NB: The executors are currently implemented as functions. A future upgrade will change the names to verbs to match, or change their implementation to classes if they're going to act as nouns.**\n",
    "\n",
    "These executors are being implemented ad hoc in the flow functions, but should probably be given single entry points and have their specific logic happen within their own files. Deeper abstractions with less coupling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from memery.loader import get_image_files, get_valid_images, archive_loader, db_loader, treemap_loader \n",
    "from memery.crafter import crafter, preproc\n",
    "from memery.encoder import image_encoder, text_encoder, image_query_encoder\n",
    "from memery.indexer import join_all, build_treemap, save_archives\n",
    "from memery.ranker import ranker, nns_to_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def index_flow(path):\n",
    "    '''Indexes images in path, returns the location of save files'''\n",
    "    root = Path(path)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Loading\n",
    "    filepaths = loader.get_image_files(root)\n",
    "    archive_db = {}\n",
    "    \n",
    "    archive_db, new_files = loader.archive_loader(filepaths, root, device)\n",
    "    print(f\"Loaded {len(archive_db)} encodings\")\n",
    "    print(f\"Encoding {len(new_files)} new images\")\n",
    "\n",
    "    # Crafting and encoding\n",
    "    crafted_files = crafter.crafter(new_files, device)\n",
    "    new_embeddings = encoder.image_encoder(crafted_files, device)\n",
    "    \n",
    "    # Reindexing\n",
    "    db = indexer.join_all(archive_db, new_files, new_embeddings)\n",
    "    print(\"Building treemap\")\n",
    "    t = indexer.build_treemap(db)\n",
    "    \n",
    "    print(f\"Saving {len(db)} encodings\")\n",
    "    save_paths = indexer.save_archives(root, t, db)\n",
    "\n",
    "    return(save_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(index_flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can index the local `images` folder to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# delete the current savefile for testing purposes\n",
    "Path('images/memery.pt').unlink()\n",
    "Path('images/memery.ann').unlink()\n",
    "\n",
    "# run the index flow. returns the path\n",
    "save_paths = index_flow('./images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert save_paths # Returns True if the path exists\n",
    "save_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def query_flow(path, query=None, image_query=None):\n",
    "    '''\n",
    "    Indexes a folder and returns file paths ranked by query.\n",
    "    \n",
    "    Parameters:\n",
    "        path (str): Folder to search\n",
    "        query (str): Search query text\n",
    "        image_query (Tensor): Search query image(s)\n",
    "\n",
    "    Returns:\n",
    "        list of file paths ranked by query\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    root = Path(path)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Check if we should re-index the files\n",
    "    print(\"Checking files\")\n",
    "    dbpath = root/'memery.pt'\n",
    "    db = loader.db_loader(dbpath, device)\n",
    "    treepath = root/'memery.ann'\n",
    "    treemap = treemap_loader(treepath)\n",
    "    filepaths = get_valid_images(root)\n",
    "\n",
    "    # # Rebuild the tree if it doesn't \n",
    "    # if treemap == None or len(db) != len(filepaths):\n",
    "    #     print('Indexing')\n",
    "    #     dbpath, treepath = index_flow(root)\n",
    "    #     treemap = loader.treemap_loader(Path(treepath))\n",
    "    #     db = loader.db_loader(dbpath, device)\n",
    "    \n",
    "    # Convert queries to vector\n",
    "    print('Converting query')\n",
    "    if image_query:\n",
    "        img = crafter.preproc(image_query)\n",
    "    if query and image_query:\n",
    "        text_vec = encoder.text_encoder(query, device)\n",
    "        image_vec = encoder.image_query_encoder(img, device)\n",
    "        query_vec = text_vec + image_vec\n",
    "    elif query:\n",
    "        query_vec = encoder.text_encoder(query, device)\n",
    "    elif image_query:\n",
    "        query_vec = encoder.image_query_encoder(img, device)\n",
    "    else:\n",
    "        print('No query!')\n",
    "\n",
    "    # Rank db by query    \n",
    "    print(f\"Searching {len(db)} images\")\n",
    "    indexes = ranker.ranker(query_vec, treemap)\n",
    "    ranked_files = ranker.nns_to_files(db, indexes)\n",
    "    \n",
    "    print(f\"Done in {time.time() - start_time} seconds\")\n",
    "    \n",
    "    return(ranked_files)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(query_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked = query_flow('./images', 'dog')\n",
    "\n",
    "print(ranked[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ranked[0] == \"images/memes/Wholesome-Meme-8.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/memes/Wholesome-Meme-8.jpg)\n",
    "\n",
    "*Then what?! What are the limitations of this system? What are its options? What configuration can i do if i'm a power user? Why did you organize things this way instead of a different way?*\n",
    "\n",
    "*This, and probably each of the following notebooks, would benefit from a small recording session where I try to explain it to an imaginary audience. So that I can get the narrative of how it works, and then arrange the code around that.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
