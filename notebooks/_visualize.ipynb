{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dimensionality Reduction\n",
    "\n",
    "One use-case for `memery` is to explore large image datasets, for cleaning and curation purposes. Sifting images by hand takes a long time, and it's near impossible to keep all the images in your mind at noce.\n",
    "\n",
    "Even with semantic search capabilities, it's hard to get an overview of all the images. CLIP sees things in many more dimensions than humans do, so no matter how many searches you run you can't be sure if you're missing some outliers you don't even know to search for.\n",
    "\n",
    "The ideal overview would be a map of all the images along all the dimensions, but we don't know how to visualize or parse 512-dimensional spaces for human brains. So we have to do dimensional reduction: find a function in some space with â‰¤ 3 dimensions that best emulates the 512-dim embeddings we have, and map that instead.\n",
    "\n",
    "The recent advance in dimensional reduction is Minimum Distortion Embedding, an abstraction over all types of embeddings like PCA, t-SNE, or k-means clustering. We can use the `pymde` library to embed them and `matplotlib` to draw the images as their own markers on the graph. We'll also need `torch` to process the tensors, and `memery` functions to process the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymde\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from memery.loader import db_loader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a database of embeddings from the local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = db_loader('images/memery.pt', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = torch.stack([v['embed'] for v in db.values()], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two methods to invoke with `pymde`: `preserve_neighbors` and `preserve_distances`. They create different textures in the final product. Let's see what each looks like on our sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mde_n = pymde.preserve_neighbors(embeds, verbose=False, device='cuda')\n",
    "mde_d = pymde.preserve_distances(embeds, verbose=False, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_n = mde_n.embed(verbose=False, snapshot_every=1)\n",
    "embed_d = mde_d.embed(verbose=False, snapshot_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymde.plot(embed_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymde.plot(embed_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mde_n.play(savepath='./graphs/mde_n.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mde_d.play(savepath='./graphs/mde_d.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert embed_n.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now I want to plot images as markers, instead of little dots. Haven't figured out yet how to merge this with `pymde.plot` functions, so I'm doing it right in matplotlib. \n",
    "\n",
    "If we just plot the images at their coordinates, they will overlap (especially on the `preserve_neighbors` plot) so eventually maybe I can normalize the x and y axes and plot things on a grid? at least a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_from_tensors(coords, image_paths, dpi=600, savefile = 'default.jpg', zoom=0.03):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.dpi = dpi\n",
    "    fig.set_size_inches(8,8)\n",
    "    \n",
    "    ax.xaxis.set_visible(False)    \n",
    "    ax.yaxis.set_visible(False)\n",
    "    \n",
    "    cc = coords.cpu()\n",
    "    x_max, y_max = cc.argmax(0)\n",
    "    x_min, y_min = cc.argmin(0)\n",
    "    \n",
    "    low = min(cc[x_min][0], cc[y_min][1])\n",
    "    high = max(cc[x_max][0], cc[y_max][1])\n",
    "    sq_lim = max(abs(low), abs(high))\n",
    "    \n",
    "    plt.xlim(low, high)\n",
    "    plt.ylim(low, high)\n",
    "    \n",
    "#     plt.xlim(-sq_lim, sq_lim)\n",
    "#     plt.ylim(-sq_lim, sq_lim)\n",
    "\n",
    "    for i, coord in tqdm(enumerate(coords)):\n",
    "        try:\n",
    "            x, y = coord\n",
    "\n",
    "            path = str(image_paths[i])\n",
    "            with open(path, 'rb') as image_file:\n",
    "                image = plt.imread(image_file)\n",
    "\n",
    "                im = OffsetImage(image, zoom=zoom, resample=False)\n",
    "                im.image.axes = ax\n",
    "                ab = AnnotationBbox(im, (x,y), frameon=False, pad=0.0,)\n",
    "                ax.add_artist(ab)\n",
    "        except SyntaxError:\n",
    "            pass\n",
    "    print(\"Drawing images as markers...\")\n",
    "    plt.savefig(savefile)\n",
    "    print(f'Saved image to {savefile}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [v['fpath'] for v in db.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefile = 'graphs/embed_n.jpg'\n",
    "\n",
    "plot_images_from_tensors(embed_n, filenames, savefile=savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefile = 'graphs/embed_d.jpg'\n",
    "\n",
    "plot_images_from_tensors(embed_d, filenames, savefile=savefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suppose it makes sense that the `preserve_neighbors` function clumps things together and the `preserve_distances` spreads them out. It's nice to see the actual distances and texture of the data, for sure. But I'd also like to be able to see them bigger, with only relative data about where they are to each other. Let's see if we can implement a normalization function and plot them again.\n",
    "\n",
    "Currently the embedding tensor is basically a list pairs of floats. Can I convert those to a set of integers that's the length of the amount of images? I don't know how to do this in matrix math so I'll try it more simply first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embed_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_list = [(float(x),float(y)) for x,y in embed_n]\n",
    "embed_dict = {k: v for k, v in zip(filenames, embed_list)}\n",
    "len(embed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeds(embed_dict):\n",
    "    sort_x = {k: v[0] for k, v in sorted(embed_dict.items(), key=lambda item: item[1][0])}\n",
    "    norm_x = {item[0]: i for i, item in enumerate(sort_x.items())}\n",
    "    \n",
    "    sort_y = {k: v[1] for k, v in sorted(embed_dict.items(), key=lambda item: item[1][1])}\n",
    "    norm_y = {item[0]: i for i, item in enumerate(sort_y.items())}\n",
    "\n",
    "    normalized_dict = {k: (norm_x[k], norm_y[k]) for k in embed_dict.keys()}\n",
    "    return(normalized_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dict = normalize_embeds(embed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(norm_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I probably could do that all in torch but right now I'm just going to pipe it back into tensors and put it through my plotting function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = torch.stack([torch.tensor([x, y]) for x, y in norm_dict.values()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_from_tensors(norms, filenames, savefile='graphs/normalized.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked!! The clusters still exist but their distances are relaxed so they can be displayed better on the graph. It's removing some information, for sure. but unclear if that is information a human needs.\n",
    "\n",
    "I wonder if it works on the `preserve_distances` method..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_list = [(float(x),float(y)) for x,y in embed_d]\n",
    "embed_dict = {k: v for k, v in zip(filenames, embed_list)}\n",
    "norm_dict = normalize_embeds(embed_dict)\n",
    "norms = torch.stack([torch.tensor([x, y]) for x, y in norm_dict.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_from_tensors(norms, filenames, savefile='graphs/normalized-d.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks okay. It reduces overall distances but keeps relative distances? Still not sure what the actionalbe difference between these two methods is. \n",
    "\n",
    "Well, it works okay for now. The next question is, how to incorporate it into a working GUI?\n",
    "\n",
    "I wonder how matplotlib does natively, for a much larger dataset. Let's see:\n",
    "\n",
    "# Large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tensors(embdgs, names):\n",
    "    embed_list = [(float(x),float(y)) for x,y in embdgs]\n",
    "    embed_dict = {k: v for k, v in zip(names, embed_list)}\n",
    "    norm_dict = normalize_embeds(embed_dict)\n",
    "    norms = torch.stack([torch.tensor([x, y]) for x, y in norm_dict.values()])\n",
    "    return(norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = db_loader('/home/mage/Pictures/memes/memery.pt', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [v['fpath'] for v in db.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clips = torch.stack([v['embed'] for v in db.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mde_lg = pymde.preserve_neighbors(clips, verbose=False, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_lg = mde_lg.embed(verbose=False, snapshot_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms_lg = normalize_tensors(embed_lg,filenames)\n",
    "len(norms_lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_from_tensors(embed_lg, filenames, savefile='graphs/normalized-lg.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Be careful here\n",
    "\n",
    "It is possible to use embeddings as target coordinates to delete sections of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = []\n",
    "for coord, img in zip(#embedding, filenames):\n",
    "    x, y = coord\n",
    "    if x < -2 or y < -1:\n",
    "        to_delete.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in to_delete:\n",
    "    imgpath = Path(img)\n",
    "    imgpath.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! A better distribution and fewer of the wrong things"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
