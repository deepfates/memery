{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loader\n",
    "\n",
    "The loader takes a directory or list of image files and checks them against database or checkpoint. If there is a saved checkpoint and the files haven't changed, it loads the checkpoint and sends the data directly to Ranker. If not, it sends them to Crafter. Ideally  it could send new images to Crafter and load dictionary of old images at the same time, without re-encoding old images.\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def slugify(filepath):\n",
    "    return f'{filepath.stem}_{str(filepath.stat().st_mtime).split(\".\")[0]}'\n",
    "\n",
    "def verify_image(f):\n",
    "    try:\n",
    "        img = Image.open(f) # open the image file\n",
    "        img.verify() # verify that it is, in fact an image\n",
    "        return(True)\n",
    "    except Exception as e:\n",
    "        print(f'Skipping bad file: {f}\\ndue to {type(e)}')\n",
    "        pass\n",
    "    \n",
    "def get_image_files(path):\n",
    "    img_extensions = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp']\n",
    "    return [(f, slugify(f)) for f in tqdm(path.rglob('*')) if f.suffix in img_extensions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [00:00, 59419.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = Path('./images')\n",
    "\n",
    "\n",
    "filepaths = get_image_files(root)\n",
    "\n",
    "len(filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(PosixPath('images/Wholesome-Meme-3.jpg'), 'Wholesome-Meme-3_1621298477'),\n",
       " (PosixPath('images/Wholesome-Meme-44.png'), 'Wholesome-Meme-44_1621298480'),\n",
       " (PosixPath('images/Wholesome-Meme-69.jpg'), 'Wholesome-Meme-69_1621298481')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepaths[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders\n",
    "\n",
    "So we have a list of paths and slugified filenames from the folder. We want to see if there's an archive, so that we don't have to recalculate tensors for images we've seen before. Then we want to pass that directly to the indexer, but send the new images through the crafter and encoder first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use the GPU, if possible, for all the pyTorch functions. But if we can't get access to it we need to fallback to CPU. Either way we call it `device` and pass it to each function in the executors that use torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `archive_loader` is only called in `indexFlow`. It takes the list of image files and the folder they're in (and the torch device), opens an archive if there is one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def archive_loader(filepaths, root, device):\n",
    "    dbpath = root/'memery.pt'\n",
    "#     dbpath_backup = root/'memery.pt'\n",
    "    db = db_loader(dbpath, device)\n",
    "    \n",
    "    current_slugs = [slug for path, slug in filepaths]    \n",
    "    archive_db = {i:db[item[0]] for i, item in enumerate(db.items()) if item[1]['slug'] in current_slugs}      \n",
    "    archive_slugs = [v['slug'] for v in archive_db.values()]\n",
    "    new_files = [(str(path), slug) for path, slug in filepaths if slug not in archive_slugs and verify_image(path)]\n",
    "    \n",
    "    return(archive_db, new_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `db_loader` takes a location and returns either the archive dictionary or an empty dictionary. Decomposed to its own function so it can be called separately from `archive_loader` or `queryFlow`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def db_loader(dbpath, device):\n",
    "\n",
    "    # check for savefile or backup and extract\n",
    "    if Path(dbpath).exists():\n",
    "        db = torch.load(dbpath, device)\n",
    "#     elif dbpath_backup.exists():\n",
    "#         db = torch.load(dbpath_backup)\n",
    "    else:\n",
    "        db = {}\n",
    "    return(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library `annoy`, [Approximate Nearest Neighbors Oh Yeah!](https://github.com/spotify/annoy) allows us to search through vector space for approximate matches instead of exact best-similarity matches. We sacrifice accuracy for speed, so we can search through tens of thousands of images in less than a thousand times the time it would take to search through tens of images. There's got to be a better way to put that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def treemap_loader(treepath):\n",
    "    treemap = AnnoyIndex(512, 'angular')\n",
    "\n",
    "    if treepath.exists():\n",
    "        treemap.load(str(treepath))\n",
    "    else:\n",
    "        treemap = None\n",
    "    return(treemap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treepath = Path('images/memery.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treemap = AnnoyIndex(512, 'angular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if treepath.exists():\n",
    "    treemap.load(str(treepath))\n",
    "else:\n",
    "    treemap = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just test on the local image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [00:00, 53092.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping bad file: images/corrupted-file.jpeg\n",
      "due to <class 'PIL.UnidentifiedImageError'>\n",
      "Skipping bad file: images/.ipynb_checkpoints/corrupted-file-checkpoint.jpeg\n",
      "due to <class 'PIL.UnidentifiedImageError'>\n"
     ]
    }
   ],
   "source": [
    "archive_db, new_files = archive_loader(get_image_files(root), root, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 0, 80)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(archive_db), len(new_files), treemap.get_n_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpath = root/'memery.pt'\n",
    "#     dbpath_backup = root/'memery.pt'\n",
    "db = db_loader(dbpath, device)\n",
    "\n",
    "current_slugs = [slug for path, slug in filepaths]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_db = {i:db[item[0]] for i, item in enumerate(db.items()) if item[1]['slug'] in current_slugs}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(archive_db)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
